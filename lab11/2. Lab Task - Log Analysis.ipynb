{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Include Libraries and Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "#import libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "appName=\"StructuredStreamingKafka\"\n",
    "#initialize the spark session\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "#get sparkcontext from the sparksession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-Case : Tracking Server Access Log <a class=\"anchor\" name=\"use-case\"></a>\n",
    "For this case, a server is going to continuously send a records of a host who is trying to access some endpoint (url) from the web server. This data will be send from a kafka producer (<code>KafkaProducer1.ipynb</code>) which is reading the data from a txt file in the dataset provided (<code>logs/access_log.txt</code>).\n",
    "\n",
    "Each line contains some valuable information such as:\n",
    "\n",
    "1. Host\n",
    "2. Timestamp\n",
    "3. HTTP method\n",
    "4. URL endpoint\n",
    "5. Status code\n",
    "6. Protocol\n",
    "7. Content Size\n",
    "\n",
    "The goal here is to perform some real time queries from this stream of data and be able to output the results in multiple ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kafka Stream \n",
    "Use the <code>readStream</code> to load data from the Kafka Producer <strong>LT1-KafkaProducer.ipynb</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "hostip = \"10.156.3.124\" #change me\n",
    "# Monitor the logs data stream for new log data\n",
    "topic = \"w11_access_log\"\n",
    "df_urls = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\") \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation <a class=\"anchor\" name=\"data-prep\"></a>\n",
    "We need to convert the data from the message in order to perform some queries. The steps to parse the data are:\n",
    "\n",
    "1. Get message as a string from <code>value</code> which is binary.\n",
    "2. Implement some regular expressions to capture specific fields in the message which is a line from the access log.\n",
    "3. Extract the values using the regular expressions to create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- host: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value of the kafka message\n",
    "log_lines = df_urls.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse out the common log format to a DataFrame\n",
    "tsExp=r'(\\d{10})\\s'\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "hostExp = r'(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "\n",
    "df_logs = log_lines.select(F.from_utc_timestamp(F.from_unixtime(regexp_extract('value', tsExp, 1)),'UTC').alias('ts'),\n",
    "                         regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         regexp_extract('value', statusExp, 1).cast('integer').alias('status'))\n",
    "\n",
    "df_logs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_logs \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations on window over event-time  <a class=\"anchor\" name=\"window\"></a>\n",
    "\n",
    "The event-time we use here is the <code>ts</code> that we have generated in the producer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong> \n",
    "Using the Window function, find the number of logs for each status in a window of 30 seconds. Set the window sliding interval to 10 seconds. Write the output to console sink.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE YOUR CODE HERE\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "windowCounts= df_logs\\\n",
    "            .withWatermark('ts', '30 seconds')\\\n",
    "            .groupBy(window(df_logs.ts,'30 seconds'),df_logs.status)\\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEND OUTPUT TO CONSOLE SINK\n",
    "query2 = windowCounts.writeStream\\\n",
    "            .outputMode(\"complete\")\\\n",
    "            .format('console')\\\n",
    "            .trigger(processingTime='10 seconds') \\\n",
    "            .option('truncate','false')\\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
